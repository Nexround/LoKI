{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_path = \"kva_result/hdf5/Qwen2.5-0.5B-Instruct/kva_mmlu.h5\"\n",
    "\n",
    "with h5py.File(hdf5_path, \"r\") as f:\n",
    "    attribution_scores = np.array(f[\"dataset\"])\n",
    "    print(attribution_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTA = 30 # Percentage of parameters to train (value between 0-100)\n",
    "\n",
    "model_name = os.path.basename(os.path.dirname(hdf5_path))\n",
    "save_dir = f\"./kva_result/pos_json/temp/{model_name}\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select trainable nodes under the guidance of Layer-Balanced Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_n(num_layers):\n",
    "    \"\"\"Split the total number into equal parts based on number of layers.\n",
    "    \n",
    "    Args:\n",
    "        num_layers: Number of layers to split across\n",
    "        \n",
    "    Returns:\n",
    "        List of equal fractions (1/num_layers for each layer)\n",
    "    \"\"\"\n",
    "    return [1 / num_layers for _ in range(num_layers)]\n",
    "\n",
    "def select_trainable_nodes(attribution_scores, quota):\n",
    "    \"\"\"Select trainable nodes based on attribution scores and quota.\n",
    "    \n",
    "    Args:\n",
    "        attribution_scores: 3D array of shape (num_inferences, num_layers, num_nodes)\n",
    "                          containing attribution scores for each node\n",
    "        quota: Percentage of parameters to train (value between 0-100)\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing indices of selected nodes for each layer\n",
    "    \"\"\"\n",
    "    num_inferences, num_layers, num_nodes = attribution_scores.shape\n",
    "    \n",
    "    # Calculate total number of trainable nodes based on quota\n",
    "    num_trainable = num_layers * num_nodes * quota / 100\n",
    "    \n",
    "    # Calculate how many nodes to select per layer (equal distribution)\n",
    "    spindle_parts = split_n(num_layers)\n",
    "    k_per_layer = list(map(lambda x: int(x * num_trainable), spindle_parts))\n",
    "    print(f\"Number of nodes to select per layer: {k_per_layer}\")\n",
    "\n",
    "    # Initialize matrix to count nodes selections across inferences\n",
    "    node_counts = np.zeros((num_layers, num_nodes), dtype=int)\n",
    "\n",
    "    # Process each inference and layer\n",
    "    for infer_idx in range(num_inferences):\n",
    "        for layer_idx in range(num_layers):\n",
    "            # Get attribution scores for current layer\n",
    "            layer_grad = attribution_scores[infer_idx, layer_idx, :]\n",
    "\n",
    "            # Apply min-max normalization\n",
    "            min_val = np.min(layer_grad)\n",
    "            max_val = np.max(layer_grad)\n",
    "            if max_val == min_val:\n",
    "                normalized = np.zeros_like(layer_grad)\n",
    "            else:\n",
    "                normalized = (layer_grad - min_val) / (max_val - min_val)\n",
    "\n",
    "            # Select indices of nodes with lowest normalized scores\n",
    "            smallest_indices = np.argsort(normalized)[: k_per_layer[layer_idx]]\n",
    "            node_counts[layer_idx, smallest_indices] += 1\n",
    "\n",
    "    # Select final trainable nodes for each layer\n",
    "    result = []\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Get and sort nodes selection counts for current layer\n",
    "        counts = node_counts[layer_idx]\n",
    "        sorted_indices = np.argsort(counts)[::-1]  # Sort in descending order\n",
    "\n",
    "        # Select top k_per_layer nodes\n",
    "        selected_indices = sorted_indices[: k_per_layer[layer_idx]]\n",
    "        result.append(selected_indices.tolist())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Execute the selection process\n",
    "result = select_trainable_nodes(attribution_scores, QUOTA)\n",
    "result_path = f\"{save_dir}/{QUOTA}.json\"\n",
    "\n",
    "# Write results to JSON file\n",
    "with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON file generated at: {result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the lowest contribution nodes globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_trainable_nodes(attribution_scores, quota):\n",
    "    \"\"\"\n",
    "    Select trainable nodes based on global attribution scores and quota.\n",
    "    \n",
    "    Args:\n",
    "        attribution_scores: 3D array of shape (num_inferences, num_layers, num_nodes)\n",
    "                          containing attribution scores for each node\n",
    "        quota: Percentage of parameters to train (value between 0-100)\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing indices of selected nodes for each layer\n",
    "    \"\"\"\n",
    "    num_inferences, num_layers, num_nodes = attribution_scores.shape\n",
    "    total_nodes = num_layers * num_nodes\n",
    "    total_trainable = int(total_nodes * quota / 100)\n",
    "    \n",
    "    # Initialize node selection count matrix\n",
    "    node_counts = np.zeros((num_layers, num_nodes), dtype=int)\n",
    "\n",
    "    # Process each inference\n",
    "    for infer_idx in range(num_inferences):\n",
    "        # Get and flatten all layer gradients for current inference\n",
    "        all_layers_grad = attribution_scores[infer_idx, :, :]\n",
    "        flattened_grad = all_layers_grad.flatten()\n",
    "\n",
    "        # Apply global min-max normalization\n",
    "        min_val = np.min(flattened_grad)\n",
    "        max_val = np.max(flattened_grad)\n",
    "        if max_val == min_val:\n",
    "            normalized = np.zeros_like(flattened_grad)\n",
    "        else:\n",
    "            normalized = (flattened_grad - min_val) / (max_val - min_val)\n",
    "\n",
    "        # Select indices of nodes with lowest global normalized scores\n",
    "        smallest_global_indices = np.argsort(normalized)[:total_trainable]\n",
    "\n",
    "        # Update node selection counts\n",
    "        for global_idx in smallest_global_indices:\n",
    "            layer_idx = global_idx // num_nodes\n",
    "            node_idx = global_idx % num_nodes\n",
    "            node_counts[layer_idx, node_idx] += 1\n",
    "\n",
    "    # Select nodes with highest selection counts\n",
    "    flattened_counts = node_counts.flatten()\n",
    "    sorted_global_indices = np.argsort(flattened_counts, kind='stable')[::-1][:total_trainable]\n",
    "\n",
    "    # Organize results by layer\n",
    "    result = [[] for _ in range(num_layers)]\n",
    "    for global_idx in sorted_global_indices:\n",
    "        layer_idx = global_idx // num_nodes\n",
    "        node_idx = global_idx % num_nodes\n",
    "        result[layer_idx].append(int(node_idx))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Execute node selection\n",
    "result = select_trainable_nodes(attribution_scores, QUOTA)\n",
    "result_path = f\"{save_dir}/GLOBAL_{QUOTA}_L.json\"\n",
    "\n",
    "# Write results to JSON file\n",
    "with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "print(f\"JSON file generated at: {result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the highest contribution nodes globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_trainable_nodes(attribution_scores, quota):\n",
    "    \"\"\"\n",
    "    Select trainable nodes based on global attribution scores and quota,\n",
    "    prioritizing nodes with highest attribution scores.\n",
    "    \n",
    "    Args:\n",
    "        attribution_scores: 3D array of shape (num_inferences, num_layers, num_nodes)\n",
    "                          containing attribution scores for each node\n",
    "        quota: Percentage of parameters to train (value between 0-100)\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing indices of selected nodes for each layer\n",
    "    \"\"\"\n",
    "    num_inferences, num_layers, num_nodes = attribution_scores.shape\n",
    "    total_nodes = num_layers * num_nodes\n",
    "    total_trainable = int(total_nodes * quota / 100)\n",
    "    \n",
    "    # Initialize node selection count matrix\n",
    "    node_counts = np.zeros((num_layers, num_nodes), dtype=int)\n",
    "\n",
    "    # Process each inference\n",
    "    for infer_idx in range(num_inferences):\n",
    "        # Get and flatten all layer gradients for current inference\n",
    "        all_layers_grad = attribution_scores[infer_idx, :, :]\n",
    "        flattened_grad = all_layers_grad.flatten()\n",
    "\n",
    "        # Apply global min-max normalization\n",
    "        min_val = np.min(flattened_grad)\n",
    "        max_val = np.max(flattened_grad)\n",
    "        if max_val == min_val:\n",
    "            normalized = np.zeros_like(flattened_grad)\n",
    "        else:\n",
    "            normalized = (flattened_grad - min_val) / (max_val - min_val)\n",
    "\n",
    "        # Select indices of nodes with HIGHEST normalized scores\n",
    "        largest_global_indices = np.argsort(-normalized)[:total_trainable]  # Negative sign for descending order\n",
    "\n",
    "        # Update node selection counts\n",
    "        for global_idx in largest_global_indices:\n",
    "            layer_idx = global_idx // num_nodes\n",
    "            node_idx = global_idx % num_nodes\n",
    "            node_counts[layer_idx, node_idx] += 1\n",
    "\n",
    "    # Select nodes with highest selection counts (original logic maintained)\n",
    "    flattened_counts = node_counts.flatten()\n",
    "    sorted_global_indices = np.argsort(flattened_counts, kind='stable')[::-1][:total_trainable]\n",
    "\n",
    "    # Organize results by layer\n",
    "    result = [[] for _ in range(num_layers)]\n",
    "    for global_idx in sorted_global_indices:\n",
    "        layer_idx = global_idx // num_nodes\n",
    "        node_idx = global_idx % num_nodes\n",
    "        result[layer_idx].append(int(node_idx))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Execute node selection\n",
    "result = select_trainable_nodes(attribution_scores, QUOTA)\n",
    "result_path = f\"{save_dir}/GLOBAL_{QUOTA}_H.json\"\n",
    "\n",
    "# Write results to JSON file\n",
    "with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "print(f\"JSON file generated at: {result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge HDF5 files\n",
    "The current code does not support multi GPU execution of KVA, so we provide a multi GPU acceleration solution: multiple GPUs execute subtasks on MMLU in parallel, and then merge their computation results together. You can use the code here to implement merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "\n",
    "def merge_h5_files(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Merge all HDF5 files' datasets from the specified folder into the output file.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the input folder.\n",
    "        output_file (str): Path to the output file.\n",
    "    \"\"\"\n",
    "    # Collect all HDF5 files (excluding the output file itself)\n",
    "    input_files = []\n",
    "    output_abspath = os.path.abspath(output_file)\n",
    "    for fname in os.listdir(input_dir):\n",
    "        fpath = os.path.abspath(os.path.join(input_dir, fname))\n",
    "        if fpath == output_abspath:\n",
    "            continue\n",
    "        if fname.endswith(('.h5', '.hdf5')):\n",
    "            input_files.append(fpath)\n",
    "    \n",
    "    if not input_files:\n",
    "        raise ValueError(\"No HDF5 files found in the input directory.\")\n",
    "    input_files.sort()  # Sort by file name\n",
    "\n",
    "    # Check the dataset structure and dtype of all files\n",
    "    ref_shape, dtype = None, None\n",
    "    total_input_rows = 0\n",
    "    for fpath in input_files:\n",
    "        with h5py.File(fpath, 'r') as f:\n",
    "            if 'dataset' not in f:\n",
    "                raise ValueError(f\"File {fpath} is missing 'dataset'\")\n",
    "            dset = f['dataset']\n",
    "            if ref_shape is None:\n",
    "                ref_shape = dset.shape[1:]\n",
    "                dtype = dset.dtype\n",
    "                total_input_rows = dset.shape[0]\n",
    "            else:\n",
    "                if dset.shape[1:] != ref_shape or dset.dtype != dtype:\n",
    "                    raise ValueError(f\"Dataset shape or dtype mismatch in file {fpath}\")\n",
    "                total_input_rows += dset.shape[0]\n",
    "\n",
    "    # Process the output file\n",
    "    with h5py.File(output_file, 'a') as f_out:\n",
    "        if 'dataset' in f_out:\n",
    "            # Check if existing dataset is compatible\n",
    "            existing_dset = f_out['dataset']\n",
    "            if existing_dset.shape[1:] != ref_shape or existing_dset.dtype != dtype:\n",
    "                raise ValueError(\"Dataset in output file is not compatible.\")\n",
    "            original_size = existing_dset.shape[0]\n",
    "            new_size = original_size + total_input_rows\n",
    "            existing_dset.resize((new_size,) + ref_shape)\n",
    "            current_pos = original_size\n",
    "        else:\n",
    "            # Create a new dataset\n",
    "            existing_dset = f_out.create_dataset(\n",
    "                'dataset',\n",
    "                shape=(total_input_rows,) + ref_shape,\n",
    "                maxshape=(None,) + ref_shape,\n",
    "                dtype=dtype\n",
    "            )\n",
    "            current_pos = 0\n",
    "\n",
    "        # Copy data file by file\n",
    "        for fpath in input_files:\n",
    "            with h5py.File(fpath, 'r') as f_in:\n",
    "                dset_in = f_in['dataset']\n",
    "                rows = dset_in.shape[0]\n",
    "                existing_dset[current_pos:current_pos+rows] = dset_in[:]\n",
    "                current_pos += rows\n",
    "\n",
    "    print(f\"Successfully merged {len(input_files)} files into {output_file}\")\n",
    "\n",
    "merge_h5_files(\"\", \"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
